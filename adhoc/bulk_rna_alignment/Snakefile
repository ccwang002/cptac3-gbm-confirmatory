import csv
from dataclasses import dataclass
import logging
from pathlib import Path
import string
from textwrap import dedent
from typing import Dict

_logger = logging.getLogger(__name__)

# List of sample names to run the pipeline
SAMPLE_TABLE_PTH = config['sample_table']
# The mapping of sample name to local file locations
FILE_MAP_PTH = config['file_map']
STAR_INDEX_FOLDER = config['star_index']  # Path to STAR index
WORKFLOW_ROOT = config['workflow_root']  # Path to this repository


@dataclass
class SampleInfo:
    """Class to keep track of the sample info."""
    unique_run_id: str
    preferred_sample_name: str
    case_id: str
    r1_fq_uuid: str
    r2_fq_uuid: str
    r1_fq_pth: Path
    r2_fq_pth: Path


# Read file map
FILE_MAP: Dict[str, Path] = {}
with open(FILE_MAP_PTH) as f:
   reader = csv.DictReader(f, dialect='excel-tab')
   for row in reader:
       FILE_MAP[row['UUID']] = Path(row['data_path'])


# Read all sample info
SAMPLE_INFO: Dict[str, SampleInfo] = {}
with open(SAMPLE_TABLE_PTH) as f:
    reader = csv.DictReader(f, dialect='excel-tab')
    for row in reader:
        run_id = row['unique_run_id']
        sample_name = row['preferred_sample_name']
        case_id = row['case_id']
        r1_fq_uuid = row['rna_raw__sample__R1__UUID']
        r2_fq_uuid = row['rna_raw__sample__R2__UUID']
        r1_fq_pth = FILE_MAP[r1_fq_uuid]
        r2_fq_pth = FILE_MAP[r2_fq_uuid]
        assert r1_fq_pth.exists()
        assert r2_fq_pth.exists()
        SAMPLE_INFO[sample_name] = SampleInfo(
            unique_run_id=run_id,
            preferred_sample_name=sample_name,
            case_id=case_id,
            r1_fq_uuid=r1_fq_uuid,
            r2_fq_uuid=r2_fq_uuid,
            r1_fq_pth=r1_fq_pth,
            r2_fq_pth=r2_fq_pth,
        )

SAMPLES = list(SAMPLE_INFO.keys())


class PartialFormatter(string.Formatter):
    """A partial string formatter.

    Example:

        >>> fmt = PartialFormatter()
        >>> fmt.format('Sale on {date} is {num_box}', date='2019-08-07')
        'Sale on 2019-08-07 is {num_box}'

    """
    def get_value(self, key, args, kwargs):
        try:
            val = super().get_value(key, args, kwargs)
        except (KeyError, IndexError):
            val = f'{{{key}}}'
        return val

_partial_fmt = PartialFormatter()


def find_sample_fastqs(wildcards):
    """Find the FASTQ file paths of a given sample."""
    sample_info = SAMPLE_INFO[wildcards.sample]
    return {
        'r1_fq': str(sample_info.r1_fq_pth),
        'r2_fq': str(sample_info.r2_fq_pth),
    }


def create_rg_line(wildcards, input):
    fq_name = Path(input.r1_fq).name
    m = re.search(r'_R[12]_\d+\.fastq\.gz$', fq_name)
    rg = fq_name[:m.start()]
    return f"ID:{rg} SM:{wildcards.sample}"


rule star_align:
    """STAR align one sample."""
    output:
        unsorted_bam=temporary('star/{sample}/Aligned.out.bam'),
        # samtools sort BAM faster
        # sorted_bam='star/{sample}/Aligned.sortedByCoord.out.bam',
        chimeric_sam=temporary('star/{sample}/Chimeric.out.sam'),
        chimeric_junction=temporary('star/{sample}/Chimeric.out.junction'),
        quant_tx_bam='star/{sample}/Aligned.toTranscriptome.out.bam',
        quant_gene_count_tab=temporary('star/{sample}/ReadsPerGene.out.tab'),
        sj_count_tab=temporary('star/{sample}/SJ.out.tab'),
    input: unpack(find_sample_fastqs)
    params:
        star_ix=STAR_INDEX_FOLDER,
        out_folder='star/{sample}/',
        outSAMattrRGline=create_rg_line
    log: 'logs/star/{sample}.log'
    threads: 8
    resources:
        io_heavy=1,
        mem_mb=lambda wildcards, attempt: 40000 + 8000 * (attempt - 1),
        tmp_mb=32000
    shell:
        "STAR "
        "--readFilesIn {input.r1_fq} {input.r2_fq} "
        # Most parameters follow GDC
        "--alignIntronMax 1000000 "
        "--alignIntronMin 20 "
        "--alignMatesGapMax 1000000 "
        "--alignSJDBoverhangMin 1 "
        "--alignSJoverhangMin 8 "
        "--alignSoftClipAtReferenceEnds Yes "

        # Follow arriba's recommendation regarding chimera parameters
        # Ref: https://arriba.readthedocs.io/en/latest/workflow/
        "--chimJunctionOverhangMin 10 "
        "--chimMainSegmentMultNmax 1 "
        "--chimOutType Junctions SeparateSAMold WithinBAM SoftClip "
        "--chimOutJunctionFormat 1 "
        "--chimSegmentMin 10 "
        "--chimScoreMin 1"
        "--chimScoreDropMax 30 "
        "--chimScoreJunctionNonGTAG 0 "
        "--chimScoreSeparation 1 "
        "--alignSJstitchMismatchNmax 5 -1 5 5 "
        "--chimSegmentReadGapMax 3 "

        "--genomeDir {params.star_ix} "
        "--genomeLoad NoSharedMemory "
        "--limitBAMsortRAM 0 "
        "--limitSjdbInsertNsj 1200000 "
        "--outFileNamePrefix {params.out_folder} "
        "--outFilterIntronMotifs None "
        "--outFilterMatchNminOverLread 0.33 "
        "--outFilterMismatchNmax 999 "
        "--outFilterMismatchNoverLmax 0.1 "
        "--outFilterMultimapNmax 20 "
        "--outFilterScoreMinOverLread 0.33 "
        "--outFilterType BySJout "
        "--outSAMattributes NH HI AS nM NM ch "
        "--outSAMattrRGline {params.outSAMattrRGline} "
        "--outSAMstrandField intronMotif "
        "--outSAMtype BAM Unsorted "
        "--outSAMunmapped Within "
        "--quantMode TranscriptomeSAM GeneCounts "
        "--readFilesCommand zcat "
        "--runThreadN {threads} "
        "--twopassMode Basic "
        "--outTmpDir $(mktemp -d)/_STARtmp "
        "> {log}"


rule samtools_index_bam:
    """Index a sorted BAM by samtools."""
    output: '{name}.bam.bai'
    input: '{name}.bam'
    resources:
        io_heavy=1
    shell: 'samtools index {input} {output}'


rule samtools_sort_star_bam:
    output: 'star/{sample}/Aligned.sortedByCoord.out.bam'
    input: rules.star_align.output.unsorted_bam
    threads: 8
    resources:
        io_heavy=1,
        mem_mb=lambda wildcards, attempt: 32000 + 8000 * (attempt - 1),
        tmp_mb=50000
    shell:
        "samtools sort "
        "--threads {threads} "
        # it uses much more memory than what's specified below
        "-m 1400M "
        "-T $(mktemp -d) "
        "-o {output} {input}"


rule samtools_sort_star_chimeric_bam:
    output: 'star/{sample}/Chimeric.out.sorted.bam'
    input: rules.star_align.output.chimeric_sam
    threads: 4
    resources:
        io_heavy=1,
        mem_mb=lambda wildcards, attempt: 4000 + 8000 * (attempt - 1),
        tmp_mb=8000
    shell:
        "samtools sort "
        "--threads {threads} -m 1400M "
        "-T $(mktemp -d) "
        "-o {output} {input}"


rule gzip_star_quant_tab:
    output: 'star/{sample}/ReadsPerGene.out.tab.gz'
    input: rules.star_align.output.quant_gene_count_tab
    shell: "gzip -9n -c {input} > {output}"


rule gzip_star_sj_tab:
    output: 'star/{sample}/SJ.pass1.out.tab.gz'
    input: rules.star_align.output.sj_count_tab
    shell: "gzip -9n -c {input} > {output}"


rule gzip_star_chimeric_junction:
    output: 'star/{sample}/Chimeric.out.junction.gz'
    input: rules.star_align.output.chimeric_junction
    shell: "gzip -9n -c {input} > {output}"


def expand_to_all_samples(patterns):
    return {
        name: expand(pattern, sample=SAMPLES)
        for name, pattern in patterns.items()
    }


rule star_align_all_samples:
    """Align all RNA-seq samples."""
    input:
        **expand_to_all_samples({ \
            "sorted_bams": rules.samtools_sort_star_bam.output[0], \
            "sorted_bam_bais": rules.samtools_sort_star_bam.output[0] + '.bai', \
            "chimeric_bams": rules.samtools_sort_star_chimeric_bam.output[0], \
            "chimeric_bam_bais": rules.samtools_sort_star_chimeric_bam.output[0] + '.bai', \
            "chimeric_junction_gzs": rules.gzip_star_chimeric_junction.output[0], \
            "quant_tx_bams": rules.star_align.output.quant_tx_bam, \
            "quant_gene_count_tab_gzs": rules.gzip_star_quant_tab.output[0], \
            "sj_count_tab_gzs": rules.gzip_star_sj_tab.output[0] \
        })


rule make_washu_output_manifest:
    """Generate the map of the custom aligned BAMs."""
    output:
        manifest='washu_rnaseq_alignment_summary.tsv'
    input: rules.star_align_all_samples.input
    run:
        result_file_tpls = {
            ('genomic', 'BAM'): rules.samtools_sort_star_bam.output[0],
            ('transcriptome', 'BAM'): rules.star_align.output.quant_tx_bam,
            ('chimeric', 'BAM'): rules.samtools_sort_star_chimeric_bam.output[0],
            ('chimeric_junction', 'TSV'): rules.gzip_star_chimeric_junction.output[0],
            ('STAR_gene_count_tab', 'TSV'): rules.gzip_star_quant_tab.output[0],
            ('splic_junction_tab', 'TSV'): rules.gzip_star_sj_tab.output[0],
        }

        columns = [
            'unique_run_id', 'preferred_sample_name', 'case_id',
            'disease', 'experimental_strategy',
            'data_format', 'result_type', 'reference',
            'data_path', 'filesize',
        ]
        with open(output.manifest, 'w') as f:
            writer = csv.writer(f, dialect='excel-tab', lineterminator='\n')
            # Write column header
            writer.writerow(columns)

            # Write all generated output files
            for run_id, sample_info in SAMPLE_INFO.items():
                for (result_type, data_format), data_pth_fmt in result_file_tpls.items():
                    data_pth = Path(data_pth_fmt.format(sample=run_id)).resolve()
                    file_size = data_pth.stat().st_size
                    writer.writerow([
                        run_id, sample_info.preferred_sample_name, sample_info.case_id,
                        'GBM', 'RNA-Seq',
                        data_format, result_type, 'hg38',
                        str(data_pth), str(file_size),
                    ])
